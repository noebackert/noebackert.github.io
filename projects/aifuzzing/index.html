<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation | My Personnal Blog - Noé Backert</title><meta name=keywords content="AI,Fuzzing,Adversarial Attacks,TranFuzz,Black-Box Attacks,Image Classification"><meta name=description content="Study and evaluation of TranFuzz for generating black-box adversarial examples on image classification models."><meta name=author content="Noé Backert"><link rel=canonical href=https://www.noe.backert.fr/projects/aifuzzing/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://www.noe.backert.fr/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.noe.backert.fr/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.noe.backert.fr/favicon-32x32.png><link rel=apple-touch-icon href=https://www.noe.backert.fr/apple-touch-icon.png><link rel=mask-icon href=https://www.noe.backert.fr/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.noe.backert.fr/projects/aifuzzing/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/css/custom.css><meta property="og:url" content="https://www.noe.backert.fr/projects/aifuzzing/"><meta property="og:site_name" content="My Personnal Blog - Noé Backert"><meta property="og:title" content="TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation"><meta property="og:description" content="Study and evaluation of TranFuzz for generating black-box adversarial examples on image classification models."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-03-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-26T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="Fuzzing"><meta property="article:tag" content="Adversarial Attacks"><meta property="article:tag" content="TranFuzz"><meta property="article:tag" content="Black-Box Attacks"><meta property="article:tag" content="Image Classification"><meta name=twitter:card content="summary"><meta name=twitter:title content="TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation"><meta name=twitter:description content="Study and evaluation of TranFuzz for generating black-box adversarial examples on image classification models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://www.noe.backert.fr/projects/"},{"@type":"ListItem","position":2,"name":"TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation","item":"https://www.noe.backert.fr/projects/aifuzzing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation","name":"TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation","description":"Study and evaluation of TranFuzz for generating black-box adversarial examples on image classification models.","keywords":["AI","Fuzzing","Adversarial Attacks","TranFuzz","Black-Box Attacks","Image Classification"],"articleBody":"Studying and evaluating TranFuzz for generating black-box adversarial examples on image classification models Project Details Project Type: Research and evaluation project on AI security Tools: PyTorch, Adversarial Robustness Toolbox (ART), Python, GitHub Duration: 2 months Team: 5 members (students and industry mentor) Article: Article Report:: Report Code: GitHub Repository Introduction Adversarial attacks can trick neural networks into making incorrect predictions through imperceptible perturbations, threatening critical systems in autonomous driving, healthcare, and cybersecurity. Traditional white-box attacks require full knowledge of the model architecture, while black-box attacks operate without this knowledge, making them realistic in practice.\nTranFuzz is a system for generating highly transferable black-box adversarial examples using domain adaptation and fuzzing techniques, providing a framework to train and test models under adversarial conditions while exploring robustness improvements.\nProject Overview The project involved:\nReviewing adversarial attacks and defenses, focusing on black-box scenarios. Studying TranFuzz, which combines Domain Adaptation (DSAN) and fuzzing for generating adversarial samples. Evaluating the robustness of models (DenseNet, AlexNet, VGG) under various attacks (FGSM, PGD, C\u0026W, ST, TranFuzz). Exploring adversarial retraining using TranFuzz-generated samples to improve robustness. Comparing results across datasets (Office31, OfficeHome) and analyzing the impact of dataset complexity on transferability and attack effectiveness. TranFuzz System TranFuzz operates through:\nDomain Adaptation using DSAN to align source and target distributions, enabling effective local substitute models. Fuzzing with neuron coverage metrics to explore under-tested areas of the model for effective perturbations. An ensemble-based seed mutation strategy to enhance adversarial sample diversity and transferability. Ensuring misclassification while preserving similarity (using a high SSIM threshold) for effective, human-imperceptible adversarial examples. Example of an adversarial attack : Figure 1: Example of a basic adversarial attack : adding some carefully selected noise to a panda image can trick an image classification model into classifying the image as a gibbon with high confidence\nEvaluation Methodology The evaluation involved:\nTraining target models on selected datasets. Training DSAN-based source models on complementary subsets. Generating adversarial examples using the TranFuzz system. Testing transferability and attack effectiveness on target models. Performing adversarial retraining with TranFuzz-generated samples to improve model robustness. Comparing results with classical attacks (FGSM, PGD, C\u0026W, ST) and with Madry’s adversarial training baseline. Figure 2: Overall fuzzing methodology diagram\nFigure 3: Adversarial retraining with augmented dataset\nResults Transferability results TranFuzz demonstrated strong transferability of adversarial examples across different models and datasets, with notable improvements over traditional methods. Figure 4: Transfer accuracy of the target model DenseNet-121 on the source model using different target datasets\nTranFuzz demonstrated strong black-box attack capabilities, reducing DenseNet accuracy from 94% to 42% in some cases. Transferability of adversarial examples increased with dataset complexity, with TranFuzz outperforming Madry retraining on harder datasets (Amazon, Product) by up to +4% in transfer accuracy. PGD attacks remained the most powerful in white-box settings, while TranFuzz was highly competitive in black-box conditions. Adversarial retraining with TranFuzz samples improved model resilience to certain attacks, but complete defense across all attack types was not achieved. Limitations included dataset size, old model architectures, and unprovided hyperparameter details, impacting reproducibility and generalization. Figure 5: DensetNet-121 result charts on different datasets\nFigure 6: Office31/Webcam result charts for different model architectures\nConclusion TranFuzz is a promising system for generating black-box adversarial attacks while enhancing model robustness via retraining, though its defensive capabilities are partial. It complements existing adversarial training strategies, with notable improvements on complex datasets and under realistic black-box conditions.\nFuture work includes:\nExpanding to advanced architectures (Transformers, EfficientNets). Testing on larger, more diverse datasets. Exploring additional attacks and defense mechanisms for holistic adversarial robustness evaluation. ","wordCount":"584","inLanguage":"en","datePublished":"2025-03-26T00:00:00Z","dateModified":"2025-03-26T00:00:00Z","author":{"@type":"Person","name":"Noé Backert"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.noe.backert.fr/projects/aifuzzing/"},"publisher":{"@type":"Organization","name":"My Personnal Blog - Noé Backert","logo":{"@type":"ImageObject","url":"https://www.noe.backert.fr/img/favicon.ico"}}}</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"],["$","$"]],displayMath:[["$$","$$"],["\\[","\\]"]]},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.noe.backert.fr/ accesskey=h title="My Personnal Blog - Noé Backert (Alt + H)">My Personnal Blog - Noé Backert</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.noe.backert.fr/ title=Home><span>Home</span></a></li><li><a href=https://www.noe.backert.fr/resume/ title=Resume><span>Resume</span></a></li><li><a href=https://www.noe.backert.fr/ctf/ title="CTF Writeups"><span>CTF Writeups</span></a></li><li><a href=https://www.noe.backert.fr/projects/ title=Projects><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.noe.backert.fr/>Home</a>&nbsp;»&nbsp;<a href=https://www.noe.backert.fr/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">TranFuzz: AI Fuzzing and Black-Box Adversarial Attack Evaluation</h1><div class=post-description>Study and evaluation of TranFuzz for generating black-box adversarial examples on image classification models.</div><div class=post-meta><span title='2025-03-26 00:00:00 +0000 UTC'>March 26, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Noé Backert</div></header><div class=post-content><h3 id=studying-and-evaluating-tranfuzz-for-generating-black-box-adversarial-examples-on-image-classification-models>Studying and evaluating TranFuzz for generating black-box adversarial examples on image classification models<a hidden class=anchor aria-hidden=true href=#studying-and-evaluating-tranfuzz-for-generating-black-box-adversarial-examples-on-image-classification-models>#</a></h3><h2 id=project-details>Project Details<a hidden class=anchor aria-hidden=true href=#project-details>#</a></h2><ul><li><strong>Project Type:</strong> Research and evaluation project on AI security</li><li><strong>Tools:</strong> PyTorch, Adversarial Robustness Toolbox (ART), Python, GitHub</li><li><strong>Duration:</strong> 2 months</li><li><strong>Team:</strong> 5 members (students and industry mentor)</li><li><strong>Article</strong>: <a href=/files/2021_TranFuzz.pdf>Article</a></li><li><strong>Report:</strong>: <a href=/files/PR_2025_Report.pdf>Report</a></li><li><strong>Code</strong>: <a href=https://github.com/noebackert/PR-ICICS>GitHub Repository</a></li></ul><hr><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Adversarial attacks can trick neural networks into making incorrect predictions through imperceptible perturbations, threatening critical systems in autonomous driving, healthcare, and cybersecurity. Traditional <strong>white-box attacks</strong> require full knowledge of the model architecture, while <strong>black-box attacks</strong> operate without this knowledge, making them realistic in practice.</p><p>TranFuzz is a system for generating <strong>highly transferable black-box adversarial examples</strong> using domain adaptation and fuzzing techniques, providing a framework to train and test models under adversarial conditions while exploring robustness improvements.</p><hr><h2 id=project-overview>Project Overview<a hidden class=anchor aria-hidden=true href=#project-overview>#</a></h2><p>The project involved:</p><ul><li>Reviewing <strong>adversarial attacks and defenses</strong>, focusing on black-box scenarios.</li><li>Studying <strong>TranFuzz</strong>, which combines <strong>Domain Adaptation (DSAN)</strong> and <strong>fuzzing</strong> for generating adversarial samples.</li><li>Evaluating the <strong>robustness of models</strong> (DenseNet, AlexNet, VGG) under various attacks (FGSM, PGD, C&amp;W, ST, TranFuzz).</li><li>Exploring <strong>adversarial retraining</strong> using TranFuzz-generated samples to improve robustness.</li><li>Comparing results across datasets (Office31, OfficeHome) and analyzing the <strong>impact of dataset complexity</strong> on transferability and attack effectiveness.</li></ul><hr><h2 id=tranfuzz-system>TranFuzz System<a hidden class=anchor aria-hidden=true href=#tranfuzz-system>#</a></h2><p>TranFuzz operates through:</p><ul><li><strong>Domain Adaptation using DSAN</strong> to align source and target distributions, enabling effective local substitute models.</li><li><strong>Fuzzing with neuron coverage metrics</strong> to explore under-tested areas of the model for effective perturbations.</li><li>An <strong>ensemble-based seed mutation strategy</strong> to enhance adversarial sample diversity and transferability.</li><li>Ensuring <strong>misclassification while preserving similarity</strong> (using a high SSIM threshold) for effective, human-imperceptible adversarial examples.</li></ul><p>Example of an adversarial attack :
<img alt=advExample loading=lazy src=/img/projects/aiFuzzing/adv.png></p><p><em>Figure 1: Example of a basic adversarial attack : adding some carefully selected noise to a panda image can trick an image classification model into classifying the image as a gibbon with high confidence</em></p><hr><h2 id=evaluation-methodology>Evaluation Methodology<a hidden class=anchor aria-hidden=true href=#evaluation-methodology>#</a></h2><p>The evaluation involved:</p><ol><li><strong>Training target models</strong> on selected datasets.</li><li><strong>Training DSAN-based source models</strong> on complementary subsets.</li><li>Generating adversarial examples using the TranFuzz system.</li><li>Testing transferability and attack effectiveness on target models.</li><li>Performing <strong>adversarial retraining</strong> with TranFuzz-generated samples to improve model robustness.</li><li>Comparing results with classical attacks (FGSM, PGD, C&amp;W, ST) and with Madry’s adversarial training baseline.</li></ol><p><img alt=trainMethodo loading=lazy src=/img/projects/aiFuzzing/trainingMethodology.png>
<em>Figure 2: Overall fuzzing methodology diagram</em></p><p><img alt=advRetraining loading=lazy src=/img/projects/aiFuzzing/advRetraining.png>
<em>Figure 3: Adversarial retraining with augmented dataset</em></p><hr><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><h3 id=transferability-results>Transferability results<a hidden class=anchor aria-hidden=true href=#transferability-results>#</a></h3><p>TranFuzz demonstrated strong transferability of adversarial examples across different models and datasets, with notable improvements over traditional methods.
<img alt=tranResults loading=lazy src=/img/projects/aiFuzzing/transfResults.png></p><p><em>Figure 4: Transfer accuracy of the target model DenseNet-121 on the source model using
different target datasets</em></p><ul><li><strong>TranFuzz demonstrated strong black-box attack capabilities</strong>, reducing DenseNet accuracy from 94% to 42% in some cases.</li><li><strong>Transferability</strong> of adversarial examples increased with dataset complexity, with TranFuzz outperforming Madry retraining on harder datasets (Amazon, Product) by up to +4% in transfer accuracy.</li><li><strong>PGD attacks remained the most powerful</strong> in white-box settings, while TranFuzz was highly competitive in black-box conditions.</li><li><strong>Adversarial retraining with TranFuzz samples</strong> improved model resilience to certain attacks, but complete defense across all attack types was not achieved.</li><li>Limitations included dataset size, old model architectures, and unprovided hyperparameter details, impacting reproducibility and generalization.</li></ul><p><img alt=densenetResults loading=lazy src=/img/projects/aiFuzzing/densenetResults.png>
<em>Figure 5: DensetNet-121 result charts on different datasets</em></p><p><img alt=webcamDatasetResults loading=lazy src=/img/projects/aiFuzzing/webcamDatasetResults.png>
<em>Figure 6: Office31/Webcam result charts for different model architectures</em></p><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>TranFuzz is a <strong>promising system for generating black-box adversarial attacks</strong> while enhancing model robustness via retraining, though its defensive capabilities are partial. It complements existing adversarial training strategies, with <strong>notable improvements on complex datasets</strong> and under realistic black-box conditions.</p><p>Future work includes:</p><ul><li>Expanding to advanced architectures (Transformers, EfficientNets).</li><li>Testing on larger, more diverse datasets.</li><li>Exploring additional attacks and defense mechanisms for holistic adversarial robustness evaluation.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.noe.backert.fr/tags/ai/>AI</a></li><li><a href=https://www.noe.backert.fr/tags/fuzzing/>Fuzzing</a></li><li><a href=https://www.noe.backert.fr/tags/adversarial-attacks/>Adversarial Attacks</a></li><li><a href=https://www.noe.backert.fr/tags/tranfuzz/>TranFuzz</a></li><li><a href=https://www.noe.backert.fr/tags/black-box-attacks/>Black-Box Attacks</a></li><li><a href=https://www.noe.backert.fr/tags/image-classification/>Image Classification</a></li></ul><nav class=paginav><a class=prev href=https://www.noe.backert.fr/ctf/drivers-shadow/><span class=title>« Prev</span><br><span>HTB CTF Driver's Shadow</span>
</a><a class=next href=https://www.noe.backert.fr/projects/lfi/><span class=title>Next »</span><br><span>Laser Fault Injection - Internship</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.noe.backert.fr/>My Personnal Blog - Noé Backert</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>